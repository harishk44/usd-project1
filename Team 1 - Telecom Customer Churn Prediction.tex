\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Team 1 - Telecom Customer Churn Prediction}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \section{Churn Prediction For Telecom
Customers}\label{churn-prediction-for-telecom-customers}

\subsubsection{By}\label{by}

\subsubsection{Harish Kapettu Acharya}\label{harish-kapettu-acharya}

\subsubsection{Shruthi AK}\label{shruthi-ak}

    \subsection{Introduction}\label{introduction}

    In the telecom sector, customers have the flexibility to select from
various service providers, often switching between them. This results in
a substantial annual churn rate of 15-25\% in this fiercely competitive
market. As retaining a customer costs is significantly less than
acquiring a new one (five to ten times less) customer retention has
become a priority over acquisition.

For established telecom companies, the primary business objective is
keeping their most profitable customers. To minimize churn, it is
crucial for these companies to identify which customers are likely to
leave. In our project, we will analyze customer data from a leading
telecom company, develop models to predict potential churn, and pinpoint
the key predictors of churn.

Churn can be defined in several ways in the telecom industry:

\textbf{\emph{Revenue-based churn}}: Customers who have not engaged in
any revenue-generating activities like using mobile internet, making
outgoing calls, or sending SMS over a certain period.

\textbf{\emph{Usage-based churn}}: Customers who have neither incoming
nor outgoing usage calls, internet, etc., over a designated period. A
drawback of this approach is that identifying churn based on a prolonged
period of inactivity, such as two months, might be ineffective as the
customer may have already switched providers by the time they are
flagged.

In this project, we will adopt the usage-based method to identify churn
based on the data collected from the telecom industry in Iran. We will
delve into how predicting and managing churn can benefit telecom
companies. The dataset is randomly collected from an Iranian telecom
company database over a period of 12 months. A total of 3150 rows and 13
columns of data, each row representing the customer information. The
attributes that are in this dataset are called out below. All of the
attributes except for attribute churn is the aggregated data of the
first 9 months.The churn labels are the state of the customers at the
end of 12 months. The three months is the designated planning gap.

    \subsubsection{About the Dataset}\label{about-the-dataset}

    \textbf{Call Failure} -\textgreater{} Number of call failures

\textbf{Complaints} -\textgreater{} Binary (0-\textgreater{} No
complaint, 1-\textgreater{} Complaint)

\textbf{Subscription length} -\textgreater{} Total months of
subscription

\textbf{Charge Amount} -\textgreater{} Ordinal attribute
(0-\textgreater{} Lowest amount, 9-\textgreater{} Highest amount)

\textbf{Seconds of Use} -\textgreater{} Total seconds of call

\textbf{Frequency of Use} -\textgreater{} Total number of calls

\textbf{Frequency of SMS} -\textgreater{} Total number of text messages

\textbf{Distinct Called Numbers} -\textgreater{} Total number of
distinct phone calls

\textbf{Age Group} -\textgreater{} Ordinal attribute (1-\textgreater{}
Younger, 5-\textgreater{} Older)

\textbf{Tariff Plan} -\textgreater{} Binary (1 -\textgreater{} Pay as
you go, 2-\textgreater{} Contractual)

\textbf{Status} -\textgreater{} Binary (1-\textgreater{} Active,
2-\textgreater{} Non-active)

\textbf{Age} -\textgreater{} Age of the customer

\textbf{Customer value} -\textgreater{} The calculated value of the
customer

\textbf{Churn} -\textgreater{} Class label (1-\textgreater{} Churn,
0-\textgreater{} Non-churn)

    \paragraph{Importing the necessary
libraries}\label{importing-the-necessary-libraries}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}

\PY{k+kn}{import} \PY{n+nn}{plotly}\PY{n+nn}{.}\PY{n+nn}{express} \PY{k}{as} \PY{n+nn}{px}
\PY{k+kn}{import} \PY{n+nn}{plotly}\PY{n+nn}{.}\PY{n+nn}{figure\PYZus{}factory} \PY{k}{as} \PY{n+nn}{ff}
\PY{k+kn}{import} \PY{n+nn}{plotly}\PY{n+nn}{.}\PY{n+nn}{graph\PYZus{}objects} \PY{k}{as} \PY{n+nn}{go}
\PY{k+kn}{from} \PY{n+nn}{plotly}\PY{n+nn}{.}\PY{n+nn}{subplots} \PY{k+kn}{import} \PY{n}{make\PYZus{}subplots}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{import} \PY{n+nn}{warnings}
\PY{n}{warnings}\PY{o}{.}\PY{n}{filterwarnings}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k+kn}{from} \PY{n+nn}{collections} \PY{k+kn}{import} \PY{n}{Counter}

\PY{k+kn}{from} \PY{n+nn}{imblearn}\PY{n+nn}{.}\PY{n+nn}{over\PYZus{}sampling} \PY{k+kn}{import} \PY{n}{SMOTE}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{StandardScaler}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{LogisticRegression}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k+kn}{import} \PY{n}{KNeighborsClassifier}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k+kn}{import} \PY{n}{RandomForestClassifier}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k+kn}{import} \PY{n}{GradientBoostingClassifier}
\PY{k+kn}{from} \PY{n+nn}{xgboost} \PY{k+kn}{import} \PY{n}{XGBClassifier}

\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{,} \PY{n}{GridSearchCV}\PY{p}{,} \PY{n}{StratifiedShuffleSplit}
\PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k+kn}{import} \PY{n}{metrics}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{accuracy\PYZus{}score}\PY{p}{,} \PY{n}{roc\PYZus{}curve}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{,} \PY{n}{classification\PYZus{}report}\PY{p}{,} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{,} \PY{n}{auc}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Data Gathering}\label{data-gathering}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Load the Customer churn dataset }
\PY{n}{churn\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{USD\PYZus{}Project\PYZus{}Dataset/Customer Churn.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} View the first five records of the dataset}
\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
   Call  Failure  Complains  Subscription  Length  Charge  Amount  \textbackslash{}
0              8          0                    38               0
1              0          0                    39               0
2             10          0                    37               0
3             10          0                    38               0
4              3          0                    38               0

   Seconds of Use  Frequency of use  Frequency of SMS  \textbackslash{}
0            4370                71                 5
1             318                 5                 7
2            2453                60               359
3            4198                66                 1
4            2393                58                 2

   Distinct Called Numbers  Age Group  Tariff Plan  Status  Age  \textbackslash{}
0                       17          3            1       1   30
1                        4          2            1       2   25
2                       24          3            1       1   30
3                       35          1            1       1   15
4                       33          1            1       1   15

   Customer Value  Churn
0         197.640      0
1          46.035      0
2        1536.520      0
3         240.020      0
4         145.805      0
\end{Verbatim}
\end{tcolorbox}
        
    \subsection{Data Preparation \&
Cleaning}\label{data-preparation-cleaning}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Renaming the columns}
\PY{n}{churn\PYZus{}data}\PY{o}{=}\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{rename}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Call  Failure}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{call\PYZus{}fail\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Complains}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{complaints\PYZus{}or\PYZus{}not\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Subscription  Length}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{subscription\PYZus{}length\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Charge  Amount}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{charge\PYZus{}amt\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                   \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Seconds of Use}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sec\PYZus{}of\PYZus{}use\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Frequency of use}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{freq\PYZus{}of\PYZus{}use\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Frequency of SMS}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{freq\PYZus{}of\PYZus{}sms\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Distinct Called Numbers}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{distinct\PYZus{}call\PYZus{}nos\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                   \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Age Group}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{age\PYZus{}gp\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tariff Plan}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tariff\PYZus{}plan\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Status}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{status\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Age}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{age\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Customer Value}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{customer\PYZus{}value\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Churn}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{churn}\PY{l+s+s2}{\PYZdq{}}\PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} To check if there is any duplicates in the dataset}
\PY{n}{duplicates} \PY{o}{=} \PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{duplicated}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The total number of duplicate data is}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{duplicates}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The shape of the dataset before removing the duplicates}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Remove the duplicates for building a better model}
\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{drop\PYZus{}duplicates}\PY{p}{(}\PY{n}{inplace}\PY{o}{=} \PY{k+kc}{True}\PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=} \PY{k+kc}{True}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The shape of the dataset after removing the duplicates}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
The total number of duplicate data is 300
The shape of the dataset before removing the duplicates (3150, 14)
The shape of the dataset after removing the duplicates (2850, 14)
    \end{Verbatim}

    \subsection{EDA (Exploratory Data
Analysis)}\label{eda-exploratory-data-analysis}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Basic info about the dataset}
\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2850 entries, 0 to 2849
Data columns (total 14 columns):
 \#   Column                Non-Null Count  Dtype
---  ------                --------------  -----
 0   call\_fail\_            2850 non-null   int64
 1   complaints\_or\_not\_    2850 non-null   int64
 2   subscription\_length\_  2850 non-null   int64
 3   charge\_amt\_           2850 non-null   int64
 4   sec\_of\_use\_           2850 non-null   int64
 5   freq\_of\_use\_          2850 non-null   int64
 6   freq\_of\_sms\_          2850 non-null   int64
 7   distinct\_call\_nos\_    2850 non-null   int64
 8   age\_gp\_               2850 non-null   int64
 9   tariff\_plan\_          2850 non-null   int64
 10  status\_               2850 non-null   int64
 11  age\_                  2850 non-null   int64
 12  customer\_value\_       2850 non-null   float64
 13  churn                 2850 non-null   int64
dtypes: float64(1), int64(13)
memory usage: 311.8 KB
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Check if there is any null values}
\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
call\_fail\_              0
complaints\_or\_not\_      0
subscription\_length\_    0
charge\_amt\_             0
sec\_of\_use\_             0
freq\_of\_use\_            0
freq\_of\_sms\_            0
distinct\_call\_nos\_      0
age\_gp\_                 0
tariff\_plan\_            0
status\_                 0
age\_                    0
customer\_value\_         0
churn                   0
dtype: int64
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Summary statistics}
\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
        call\_fail\_  complaints\_or\_not\_  subscription\_length\_  charge\_amt\_  \textbackslash{}
count  2850.000000         2850.000000           2850.000000  2850.000000
mean      7.802456            0.080702             32.452982     0.974737
std       7.326172            0.272424              8.723075     1.550618
min       0.000000            0.000000              3.000000     0.000000
25\%       1.000000            0.000000             29.000000     0.000000
50\%       6.000000            0.000000             35.000000     0.000000
75\%      12.000000            0.000000             38.000000     2.000000
max      36.000000            1.000000             47.000000    10.000000

        sec\_of\_use\_  freq\_of\_use\_  freq\_of\_sms\_  distinct\_call\_nos\_  \textbackslash{}
count   2850.000000   2850.000000   2850.000000         2850.000000
mean    4534.243158     70.484912     73.789825           23.870526
std     4199.712303     57.401512    112.062397           17.193929
min        0.000000      0.000000      0.000000            0.000000
25\%     1458.750000     28.000000      7.000000           11.000000
50\%     3041.000000     54.500000     22.000000           21.000000
75\%     6500.000000     96.000000     88.000000           34.000000
max    17090.000000    255.000000    522.000000           97.000000

           age\_gp\_  tariff\_plan\_      status\_         age\_  customer\_value\_  \textbackslash{}
count  2850.000000   2850.000000  2850.000000  2850.000000      2850.000000
mean      2.835088      1.080351     1.240000    31.077193       474.990367
std       0.893503      0.271883     0.427158     8.861934       514.442198
min       1.000000      1.000000     1.000000    15.000000         0.000000
25\%       2.000000      1.000000     1.000000    25.000000       117.527500
50\%       3.000000      1.000000     1.000000    30.000000       232.520000
75\%       3.000000      1.000000     1.000000    30.000000       790.080000
max       5.000000      2.000000     2.000000    55.000000      2165.280000

             churn
count  2850.000000
mean      0.156491
std       0.363384
min       0.000000
25\%       0.000000
50\%       0.000000
75\%       0.000000
max       1.000000
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Shape of the dataset}
\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(2850, 14)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Create the count plot}
\PY{n}{fig} \PY{o}{=} \PY{n}{px}\PY{o}{.}\PY{n}{histogram}\PY{p}{(}\PY{n}{churn\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{churn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{x}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{churn}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{churn}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{category\PYZus{}orders}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{churn}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{color\PYZus{}discrete\PYZus{}sequence}\PY{o}{=}\PY{n}{px}\PY{o}{.}\PY{n}{colors}\PY{o}{.}\PY{n}{qualitative}\PY{o}{.}\PY{n}{Set3}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Update layout for styling}
\PY{n}{fig}\PY{o}{.}\PY{n}{update\PYZus{}layout}\PY{p}{(}
    \PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Churn Count}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{xaxis\PYZus{}title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Churn}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{yaxis\PYZus{}title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Count}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{xaxis}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}\PY{n}{tickmode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{array}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{tickvals}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n+nb}{range}\PY{o}{=}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{1.5}\PY{p}{]}\PY{p}{)}\PY{p}{,}
    \PY{n}{template}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{plotly\PYZus{}dark}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{width}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,}
    \PY{n}{height}\PY{o}{=}\PY{l+m+mi}{400}
\PY{p}{)}

\PY{c+c1}{\PYZsh{} Show the plot}
\PY{n}{fig}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_17_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The dataset is highly imbalanced with respect to the target variable
`Churn'. It contains 2404 instances of non-churned customers (labeled as
0) and 446 instances of churned customers (labeled as 1). This imbalance
poses a challenge for accurate model training and necessitates the use
of techniques such as SMOTE to address the class disparity.

    \subsubsection{Univariate Analysis}\label{univariate-analysis}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Frequency distribution of all the columns}
\PY{c+c1}{\PYZsh{} Determine the number of rows and columns for the subplots}
\PY{n}{num\PYZus{}columns} \PY{o}{=} \PY{l+m+mi}{3}
\PY{n}{num\PYZus{}rows} \PY{o}{=} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{n}{num\PYZus{}columns}

\PY{c+c1}{\PYZsh{} Create a subplot grid}
\PY{n}{fig} \PY{o}{=} \PY{n}{make\PYZus{}subplots}\PY{p}{(}\PY{n}{rows}\PY{o}{=}\PY{n}{num\PYZus{}rows}\PY{p}{,} \PY{n}{cols}\PY{o}{=}\PY{n}{num\PYZus{}columns}\PY{p}{,} \PY{n}{subplot\PYZus{}titles}\PY{o}{=}\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Add a histogram for each column}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{column} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{:}
    \PY{n}{row} \PY{o}{=} \PY{n}{i} \PY{o}{/}\PY{o}{/} \PY{n}{num\PYZus{}columns} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{n}{col} \PY{o}{=} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{n}{num\PYZus{}columns} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}trace}\PY{p}{(}\PY{n}{go}\PY{o}{.}\PY{n}{Histogram}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{churn\PYZus{}data}\PY{p}{[}\PY{n}{column}\PY{p}{]}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{n}{column}\PY{p}{)}\PY{p}{,} \PY{n}{row}\PY{o}{=}\PY{n}{row}\PY{p}{,} \PY{n}{col}\PY{o}{=}\PY{n}{col}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Update layout}
\PY{n}{fig}\PY{o}{.}\PY{n}{update\PYZus{}layout}\PY{p}{(}
    \PY{n}{title\PYZus{}text}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histograms for all columns}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{height}\PY{o}{=}\PY{l+m+mi}{500} \PY{o}{*} \PY{n}{num\PYZus{}rows}\PY{p}{,}
    \PY{n}{width}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,}
    \PY{n}{template}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{plotly\PYZus{}dark}\PY{l+s+s2}{\PYZdq{}}
\PY{p}{)}

\PY{c+c1}{\PYZsh{} Show the plot}
\PY{n}{fig}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_20_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Univariate analysis involves examining each feature individually to
understand its distribution, central tendency, and dispersion. This
analysis is crucial for identifying potential issues such as skewness,
outliers, and other anomalies in the data.

During the univariate analysis of our dataset, we observed that many
features exhibit right skewness, and subscription length with left
skewness, indicating that the distribution of these features has a long
tail on the right side and left side respectively. This skewness is
primarily due to the presence of outliers---data points that deviate
significantly from the majority of the data.

To address the skewness and ensure our model performs optimally, we can
retain the outliers but addressing their impact through scaling to
prevent them from disproportionately affecting the model.

    \subsubsection{Bivariate Analysis}\label{bivariate-analysis}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Comparing all the columns with the target variable \PYZsq{}Churn\PYZsq{}}
\PY{c+c1}{\PYZsh{} Determine the number of rows and columns for the subplots}
\PY{n}{num\PYZus{}columns} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{num\PYZus{}rows} \PY{o}{=} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{n}{num\PYZus{}columns}

\PY{c+c1}{\PYZsh{} Create a subplot grid}
\PY{n}{fig} \PY{o}{=} \PY{n}{make\PYZus{}subplots}\PY{p}{(}\PY{n}{rows}\PY{o}{=}\PY{n}{num\PYZus{}rows}\PY{p}{,} \PY{n}{cols}\PY{o}{=}\PY{n}{num\PYZus{}columns}\PY{p}{,} \PY{n}{subplot\PYZus{}titles}\PY{o}{=}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Churn vs }\PY{l+s+si}{\PYZob{}}\PY{n}{col}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}} \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{columns} \PY{k}{if} \PY{n}{col} \PY{o}{!=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{churn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Add a box plot for each column against Churn}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{column} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{column} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{churn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
        \PY{k}{continue}
    \PY{n}{row} \PY{o}{=} \PY{n}{i} \PY{o}{/}\PY{o}{/} \PY{n}{num\PYZus{}columns} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{n}{col} \PY{o}{=} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{n}{num\PYZus{}columns} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}trace}\PY{p}{(}
        \PY{n}{go}\PY{o}{.}\PY{n}{Box}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{n}{churn\PYZus{}data}\PY{p}{[}\PY{n}{column}\PY{p}{]}\PY{p}{,} \PY{n}{x}\PY{o}{=}\PY{n}{churn\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{churn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{n}{column}\PY{p}{,} \PY{n}{boxmean}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
        \PY{n}{row}\PY{o}{=}\PY{n}{row}\PY{p}{,} \PY{n}{col}\PY{o}{=}\PY{n}{col}
    \PY{p}{)}

\PY{c+c1}{\PYZsh{} Update layout}
\PY{n}{fig}\PY{o}{.}\PY{n}{update\PYZus{}layout}\PY{p}{(}
    \PY{n}{title\PYZus{}text}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Box Plots of Churn vs Other Variables}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{height}\PY{o}{=}\PY{l+m+mi}{500} \PY{o}{*} \PY{n}{num\PYZus{}rows}\PY{p}{,}
    \PY{n}{width}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,}
    \PY{n}{template}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{plotly\PYZus{}dark}\PY{l+s+s2}{\PYZdq{}}
\PY{p}{)}

\PY{c+c1}{\PYZsh{} Show the plot}
\PY{n}{fig}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_23_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Bivariate analysis involves examining the relationship between each
feature and the target variable, in this case, churn. This analysis
helps us understand how different features influence the likelihood of
customer churn. Box plots were utilized to visualize these
relationships, providing insights into the distribution of features for
churned and non-churned customers.

Outliers are present in several features, as indicated by the extended
whiskers and individual points beyond the whiskers in the box plots.
These outliers can represent extreme usage patterns or anomalies in
customer behavior.

The observed relationships indicate that certain features, such as Call
Failure, Subscription Length, Charge Amount, Seconds of Use, and
Frequency of Use, Complaints, Status, Tariff plan are potentially strong
predictors of churn. These features show clear differences between
churned and non-churned groups, which can enhance the predictive power
of the model.

    \subsubsection{Multivariate Analysis}\label{multivariate-analysis}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Calculate correlation}
\PY{n}{churn\PYZus{}data\PYZus{}corr} \PY{o}{=} \PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Create heatmap using plotly}
\PY{n}{fig} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Figure}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{go}\PY{o}{.}\PY{n}{Heatmap}\PY{p}{(}
                   \PY{n}{z}\PY{o}{=}\PY{n}{churn\PYZus{}data\PYZus{}corr}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                   \PY{n}{x}\PY{o}{=}\PY{n}{churn\PYZus{}data\PYZus{}corr}\PY{o}{.}\PY{n}{columns}\PY{p}{,}
                   \PY{n}{y}\PY{o}{=}\PY{n}{churn\PYZus{}data\PYZus{}corr}\PY{o}{.}\PY{n}{index}\PY{p}{,}
                   \PY{n}{colorscale}\PY{o}{=}\PY{n}{px}\PY{o}{.}\PY{n}{colors}\PY{o}{.}\PY{n}{diverging}\PY{o}{.}\PY{n}{Tealrose}\PY{p}{,}
                   \PY{n}{zmin}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{zmax}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
                   \PY{n}{hoverongaps}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}
                   \PY{n}{colorbar}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}\PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Correlation Coefficient}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                  \PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Update layout}
\PY{n}{fig}\PY{o}{.}\PY{n}{update\PYZus{}layout}\PY{p}{(}
    \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Correlation Matrix}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{xaxis\PYZus{}nticks}\PY{o}{=}\PY{l+m+mi}{36}\PY{p}{,}
    \PY{n}{height}\PY{o}{=}\PY{l+m+mi}{600}\PY{p}{,}
    \PY{n}{width}\PY{o}{=}\PY{l+m+mi}{800}\PY{p}{,}
    \PY{n}{template}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{plotly\PYZus{}dark}\PY{l+s+s2}{\PYZdq{}}
\PY{p}{)}

\PY{c+c1}{\PYZsh{} Show the plot}
\PY{n}{fig}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Multivariate analysis involves examining the relationships between
multiple features simultaneously to understand how they interact and
influence the target variable. This analysis is crucial for feature
selection and engineering, ensuring that the model captures the most
relevant information.

A correlation matrix is a table that shows the correlation coefficients
between pairs of features. The correlation coefficient values range from
-1 to 1, indicating the strength and direction of the relationship:

+1: Perfect positive correlation

-1: Perfect negative correlation

0: No correlation

A heat map is a visual representation of the correlation matrix, where
colors represent the magnitude of the correlation coefficients. This
visualization helps quickly identify strong correlations and patterns
among features.

Based on the correlation analysis, we decided to drop the ``Age'' and
``Frequency of SMS'' features from the dataset. This decision helps in
reducing multicollinearity, simplifying the model, and potentially
improving its performance. By retaining the most relevant features, we
ensure that the model remains robust, interpretable, and effective in
predicting customer churn.

Even though ``Seconds of Use'' and ``Frequency of Use'' are highly
correlated, retaining both aligns with our goal of building a
comprehensive churn prediction model. Their combined inclusion allows us
to capture nuanced aspects of customer behavior that are crucial for
understanding and predicting churn in our telecom industry.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{churn\PYZus{}data} \PY{o}{=} \PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{age\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{freq\PYZus{}of\PYZus{}sms\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1} \PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(2850, 12)
\end{Verbatim}
\end{tcolorbox}
        
    Our dataset consists of 2850 data points. During initial analysis, we
identified the presence of outliers. Given the relatively small size of
the dataset, we made a strategic decision to retain these outliers.

Outliers often carry valuable information about extreme variations and
edge cases in the data. Removing them without careful consideration
might result in the loss of important insights, which could be crucial
for the performance and robustness of our model.

In small datasets like ours, every data point counts. Eliminating
outliers can significantly reduce the dataset size, leading to potential
bias and overfitting issues. Retaining outliers ensures that we leverage
the full scope of available data, capturing the inherent variability of
the dataset.

To address the influence of outliers without removing them, we utilize
scaling techniques. Scaling methods such as StandardScaler normalize the
data, adjusting the values to a common range (-3 to 3). This process
ensures that outliers do not disproportionately impact the model
training, as all features are brought to a similar scale.

Hence, retaining outliers in a small dataset and addressing them through
scaling is a balanced approach that preserves data integrity and ensures
robust model performance. This strategy allows us to leverage the full
dataset, capturing all variations and insights without letting extreme
values skew the results.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Create the pairplot using plotly}
\PY{n}{fig} \PY{o}{=} \PY{n}{px}\PY{o}{.}\PY{n}{scatter\PYZus{}matrix}\PY{p}{(}\PY{n}{churn\PYZus{}data}\PY{p}{,}
                        \PY{n}{dimensions}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{call\PYZus{}fail\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subscription\PYZus{}length\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{charge\PYZus{}amt\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sec\PYZus{}of\PYZus{}use\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{freq\PYZus{}of\PYZus{}use\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{distinct\PYZus{}call\PYZus{}nos\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{customer\PYZus{}value\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                        \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{churn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  \PY{c+c1}{\PYZsh{} Color by the target variable}
                        \PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Customer Churn Pairplot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                        \PY{n}{labels}\PY{o}{=}\PY{p}{\PYZob{}}\PY{n}{col}\PY{p}{:} \PY{n}{col}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{columns} \PY{k}{if} \PY{n}{col} \PY{o}{!=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{churn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{,}
                        \PY{n}{height}\PY{o}{=}\PY{l+m+mi}{1200}\PY{p}{,}  \PY{c+c1}{\PYZsh{} Further increase height for better visibility}
                        \PY{n}{width}\PY{o}{=}\PY{l+m+mi}{1200}\PY{p}{,}   \PY{c+c1}{\PYZsh{} Further increase width for better visibility}
                        \PY{n}{color\PYZus{}continuous\PYZus{}scale}\PY{o}{=}\PY{n}{px}\PY{o}{.}\PY{n}{colors}\PY{o}{.}\PY{n}{sequential}\PY{o}{.}\PY{n}{Viridis} \PY{c+c1}{\PYZsh{} Custom color scale}
                        \PY{p}{)}
\PY{c+c1}{\PYZsh{} Customize the plot}
\PY{n}{fig}\PY{o}{.}\PY{n}{update\PYZus{}traces}\PY{p}{(}\PY{n}{marker}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Reduce marker size for clarity}

\PY{c+c1}{\PYZsh{} Rotate x\PYZhy{}axis labels}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{fig}\PY{o}{.}\PY{n}{data}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{fig}\PY{o}{.}\PY{n}{update\PYZus{}xaxes}\PY{p}{(}\PY{n}{tickangle}\PY{o}{=}\PY{l+m+mi}{45}\PY{p}{,} \PY{n}{row}\PY{o}{=}\PY{p}{(}\PY{n}{i} \PY{o}{/}\PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{col}\PY{o}{=}\PY{p}{(}\PY{n}{i} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{fig}\PY{o}{.}\PY{n}{update\PYZus{}yaxes}\PY{p}{(}\PY{n}{tickangle}\PY{o}{=}\PY{l+m+mi}{45}\PY{p}{,} \PY{n}{row}\PY{o}{=}\PY{p}{(}\PY{n}{i} \PY{o}{/}\PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{col}\PY{o}{=}\PY{p}{(}\PY{n}{i} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Show the plot}
\PY{n}{fig}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Pairplot is used to visualize relationships between pairs of variables
in a dataset. It provides a comprehensive view of how different
variables interact with each other, showing both distributions and
correlations.

While analyzing the numerical columns, pair plot demostrates the
distribution among the different variables and their correlation. Since,
all these attributes contribute to the final churn prediction, we are
retaining it.

    \subsubsection{Sampling}\label{sampling}

    \paragraph{1. Random sampling}\label{random-sampling}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}split the data}
\PY{n}{X} \PY{o}{=} \PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{11}\PY{p}{]}
\PY{c+c1}{\PYZsh{}store the output variable in y}
\PY{n}{y} \PY{o}{=} \PY{n}{churn\PYZus{}data}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{11}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
((2850, 11), (2850,))
\end{Verbatim}
\end{tcolorbox}
        
    For the initial setup of our modeling process, we utilized random
sampling with a train-test split:

Methodology: We randomly divided the dataset into training (80\%) and
testing (20\%) sets using train\_test\_split from scikit-learn.

Purpose: This approach allows us to train the model on a large portion
of the data while preserving a separate portion for evaluating its
performance on unseen data. Random sampling ensures that both training
and testing sets maintain a similar distribution of the target variable.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Scaling data t0 \PYZhy{}3 to 3}
\PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{scaler}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{n}{X} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{c+c1}{\PYZsh{} split data to X train/test and Y train/test}
\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.20}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,} \PY{n}{stratify} \PY{o}{=} \PY{n}{y}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{Y\PYZus{}test}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
((2280, 11), (570, 11), (2280,), (570,))
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{2. SMOTE}\label{smote}

    Given the imbalance in our target variable, we applied SMOTE to address
this issue:

Methodology: SMOTE is a technique that synthesizes new minority class
instances (churned customers) to create a more balanced dataset.

Purpose: By oversampling the minority class, SMOTE helps mitigate the
bias towards the majority class during model training. This improves the
model's ability to recognize patterns and make accurate predictions for
the minority class.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sm} \PY{o}{=} \PY{n}{SMOTE}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,} \PY{n}{sampling\PYZus{}strategy} \PY{o}{=} \PY{l+m+mf}{1.0}\PY{p}{)}
\PY{n}{X\PYZus{}smote}\PY{p}{,} \PY{n}{y\PYZus{}smote}  \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{fit\PYZus{}resample}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\PY{n}{X\PYZus{}smote\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}smote}\PY{p}{,} \PY{n}{y\PYZus{}smote}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.20}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,} \PY{n}{stratify} \PY{o}{=} \PY{n}{y\PYZus{}smote}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}smote\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{X\PYZus{}smote\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
((3846, 11), (962, 11), (3846,), (962,))
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Checking if the data is balanced or not}
\PY{n}{counter} \PY{o}{=} \PY{n}{Counter}\PY{p}{(}\PY{n}{y\PYZus{}smote}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{counter}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Counter(\{0: 2404, 1: 2404\})
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}scale smote data}
\PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{scaler}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}smote}\PY{p}{)}
\PY{n}{X\PYZus{}scaled\PYZus{}smote} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}smote}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} split data to X train/test and Y train/test}
\PY{n}{X\PYZus{}smote\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}scaled\PYZus{}smote}\PY{p}{,} \PY{n}{y\PYZus{}smote}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.20}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,} \PY{n}{stratify} \PY{o}{=} \PY{n}{y\PYZus{}smote}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}smote\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{X\PYZus{}smote\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
((3846, 11), (962, 11), (3846,), (962,))
\end{Verbatim}
\end{tcolorbox}
        
    \subsection{Model Selection}\label{model-selection}

    \subsubsection{1. Logistic Regression}\label{logistic-regression}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Model training with scaled input data}
\PY{n}{log\PYZus{}model} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{p}{)}
\PY{n}{log\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model prediction}
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{log\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model evaluation}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Classification Report:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Confusion Matrix:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ROC\PYZhy{}AUC Score:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.9

Classification Report:
               precision    recall  f1-score   support

           0       0.91      0.98      0.94       481
           1       0.81      0.47      0.60        89

    accuracy                           0.90       570
   macro avg       0.86      0.73      0.77       570
weighted avg       0.89      0.90      0.89       570


Confusion Matrix:
 [[471  10]
 [ 47  42]]

ROC-AUC Score:
 0.725560045784765
    \end{Verbatim}

    \textbf{\emph{Model Performance Metrics: Our logistic regression model
achieved an accuracy of 90\%, precision of 0.89, recall of 0.90,
f1-score 0.89 and an ROC-AUC score of 0.725560045784765.}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Model training with scaled smote input data}
\PY{n}{log\PYZus{}smote\PYZus{}model} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{p}{)}
\PY{n}{log\PYZus{}smote\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}smote\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model prediction}
\PY{n}{y\PYZus{}smote\PYZus{}pred} \PY{o}{=} \PY{n}{log\PYZus{}smote\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}smote\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model evaluation}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Classification Report:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Confusion Matrix:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ROC\PYZhy{}AUC Score:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.8378378378378378

Classification Report:
               precision    recall  f1-score   support

           0       0.84      0.83      0.84       481
           1       0.84      0.84      0.84       481

    accuracy                           0.84       962
   macro avg       0.84      0.84      0.84       962
weighted avg       0.84      0.84      0.84       962


Confusion Matrix:
 [[401  80]
 [ 76 405]]

ROC-AUC Score:
 0.8378378378378378
    \end{Verbatim}

    \textbf{\emph{Model Performance Metrics: Our logistic regression model
using SMOTE achieved an accuracy of 83.78\%, precision of 0.84, recall
of 0.84, f1-score 0.84 and an ROC-AUC score of 0.8378378378378378.}}

    \subsubsection{2. K Nearest Neighbours
(KNN)}\label{k-nearest-neighbours-knn}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Model training with scaled input data}
\PY{n}{k} \PY{o}{=} \PY{l+m+mi}{5}  \PY{c+c1}{\PYZsh{} Number of neighbors}
\PY{n}{knn} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{n}{k}\PY{p}{)}
\PY{n}{knn}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model prediction}
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{knn}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model evaluation}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Classification Report:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Confusion Matrix:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ROC\PYZhy{}AUC Score:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.9473684210526315

Classification Report:
               precision    recall  f1-score   support

           0       0.97      0.97      0.97       481
           1       0.83      0.83      0.83        89

    accuracy                           0.95       570
   macro avg       0.90      0.90      0.90       570
weighted avg       0.95      0.95      0.95       570


Confusion Matrix:
 [[466  15]
 [ 15  74]]

ROC-AUC Score:
 0.9001378214861361
    \end{Verbatim}

    \textbf{\emph{Model Performance Metrics: Our K Nearest Neighbours model
achieved an accuracy of 94.73\%, precision of 0.95, recall of 0.95,
f1-score 0.95 and an ROC-AUC score of 0.9001378214861361.}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Model training with scaled smote input data}
\PY{n}{k} \PY{o}{=} \PY{l+m+mi}{5}  \PY{c+c1}{\PYZsh{} Number of neighbors}
\PY{n}{knn\PYZus{}smote} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{n}{k}\PY{p}{)}
\PY{n}{knn\PYZus{}smote}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}smote\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model prediction}
\PY{n}{y\PYZus{}smote\PYZus{}pred} \PY{o}{=} \PY{n}{knn\PYZus{}smote}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}smote\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model evaluation}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Classification Report:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Confusion Matrix:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ROC\PYZhy{}AUC Score:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.9542619542619543

Classification Report:
               precision    recall  f1-score   support

           0       0.98      0.93      0.95       481
           1       0.93      0.98      0.96       481

    accuracy                           0.95       962
   macro avg       0.96      0.95      0.95       962
weighted avg       0.96      0.95      0.95       962


Confusion Matrix:
 [[445  36]
 [  8 473]]

ROC-AUC Score:
 0.9542619542619543
    \end{Verbatim}

    \textbf{\emph{Model Performance Metrics: Our K Nearest Neighbours model
using SMOTE data achieved an accuracy of 95.42\%, precision of 0.96,
recall of 0.95, f1-score 0.95 and an ROC-AUC score of
0.9542619542619543.}}

    \subsubsection{3. Random Forest
Classifier}\label{random-forest-classifier}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Define the parameter grid for grid search}
\PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{,} \PY{l+m+mi}{300}\PY{p}{]}\PY{p}{,}  \PY{c+c1}{\PYZsh{} Number of trees in the forest}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{,}  \PY{c+c1}{\PYZsh{} Maximum depth of the trees}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,}  \PY{c+c1}{\PYZsh{} Minimum number of samples required to split a node}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}  \PY{c+c1}{\PYZsh{} Minimum number of samples required at each leaf node}
\PY{p}{\PYZcb{}}
\PY{c+c1}{\PYZsh{} Create a Random Forest classifier}
\PY{n}{rf\PYZus{}classifier} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Perform grid search cross\PYZhy{}validation}
\PY{n}{grid\PYZus{}search\PYZus{}rf} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{estimator}\PY{o}{=}\PY{n}{rf\PYZus{}classifier}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model training with scaled input data}
\PY{n}{grid\PYZus{}search\PYZus{}rf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Get the best estimator and its hyperparameters}
\PY{n}{best\PYZus{}rf\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search\PYZus{}rf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
\PY{n}{best\PYZus{}params\PYZus{}rf\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search\PYZus{}rf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best Parameters:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{best\PYZus{}params\PYZus{}rf\PYZus{}classifier}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model Prediction using the best estimator}
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{best\PYZus{}rf\PYZus{}classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model evaluation}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Classification Report:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Confusion Matrix:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ROC\PYZhy{}AUC Score:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best Parameters: \{'max\_depth': None, 'min\_samples\_leaf': 1, 'min\_samples\_split':
10, 'n\_estimators': 200\}
Accuracy: 0.9596491228070175

Classification Report:
               precision    recall  f1-score   support

           0       0.96      0.99      0.98       481
           1       0.93      0.80      0.86        89

    accuracy                           0.96       570
   macro avg       0.95      0.89      0.92       570
weighted avg       0.96      0.96      0.96       570


Confusion Matrix:
 [[476   5]
 [ 18  71]]

ROC-AUC Score:
 0.8936788992968768
    \end{Verbatim}

    \textbf{\emph{Model Performance Metrics: Our Random Forest Classifier
achieved an accuracy of 95.96\%, precision of 0.96, recall of 0.96,
f1-score 0.96 and an ROC-AUC score of 0.8936788992968768.}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Model training with scaled smote input data}
\PY{n}{grid\PYZus{}search\PYZus{}rf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}smote\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Get the best estimator and its hyperparameters}
\PY{n}{best\PYZus{}rf\PYZus{}smote\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search\PYZus{}rf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
\PY{n}{best\PYZus{}params\PYZus{}rf\PYZus{}smote\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search\PYZus{}rf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best Parameters:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{best\PYZus{}params\PYZus{}rf\PYZus{}smote\PYZus{}classifier}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model Prediction using the best estimator}
\PY{n}{y\PYZus{}smote\PYZus{}pred} \PY{o}{=} \PY{n}{best\PYZus{}rf\PYZus{}smote\PYZus{}classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}smote\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model evaluation}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Classification Report:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Confusion Matrix:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ROC\PYZhy{}AUC Score:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best Parameters: \{'max\_depth': 20, 'min\_samples\_leaf': 1, 'min\_samples\_split':
2, 'n\_estimators': 300\}
Accuracy: 0.974012474012474

Classification Report:
               precision    recall  f1-score   support

           0       0.98      0.96      0.97       481
           1       0.97      0.98      0.97       481

    accuracy                           0.97       962
   macro avg       0.97      0.97      0.97       962
weighted avg       0.97      0.97      0.97       962


Confusion Matrix:
 [[464  17]
 [  8 473]]

ROC-AUC Score:
 0.9740124740124741
    \end{Verbatim}

    \textbf{\emph{Model Performance Metrics: Our Random Forest Classifier
using SMOTE data achieved an accuracy of 97.40\%, precision of 0.97,
recall of 0.97, f1-score 0.97 and an ROC-AUC score of
0.9740124740124741.}}

    \subsubsection{4. Gradient Boosting Decision Tree -
Classifier}\label{gradient-boosting-decision-tree---classifier}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Define the parameter grid for grid search}
\PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{]}
\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Create a Gradient Boosting classifier}
\PY{n}{gb\PYZus{}classifier} \PY{o}{=} \PY{n}{GradientBoostingClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Perform grid search cross\PYZhy{}validation}
\PY{n}{grid\PYZus{}search\PYZus{}gb} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{estimator}\PY{o}{=}\PY{n}{gb\PYZus{}classifier}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{grid\PYZus{}search\PYZus{}gb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Get the best estimator and its hyperparameters}
\PY{n}{best\PYZus{}gb\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search\PYZus{}gb}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
\PY{n}{best\PYZus{}params\PYZus{}gb\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search\PYZus{}gb}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best Parameters:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{best\PYZus{}params\PYZus{}gb\PYZus{}classifier}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model prediction using the best estimator}
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{best\PYZus{}gb\PYZus{}classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model evaluation}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Classification Report:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Confusion Matrix:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ROC\PYZhy{}AUC Score:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best Parameters: \{'learning\_rate': 0.2, 'max\_depth': 7, 'n\_estimators': 200\}
Accuracy: 0.9701754385964912

Classification Report:
               precision    recall  f1-score   support

           0       0.98      0.99      0.98       481
           1       0.93      0.88      0.90        89

    accuracy                           0.97       570
   macro avg       0.95      0.93      0.94       570
weighted avg       0.97      0.97      0.97       570


Confusion Matrix:
 [[475   6]
 [ 11  78]]

ROC-AUC Score:
 0.9319652409540049
    \end{Verbatim}

    \textbf{\emph{Model Performance Metrics: Our Gradient Boosting Decision
Tree - Classifier achieved an accuracy of 97.01\%, precision of 0.97,
recall of 0.97, f1-score 0.97 and an ROC-AUC score of
0.9319652409540049.}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Model training with scaled smote input data}
\PY{n}{grid\PYZus{}search\PYZus{}gb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}smote\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Get the best estimator and its hyperparameters}
\PY{n}{best\PYZus{}gb\PYZus{}smote\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search\PYZus{}gb}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
\PY{n}{best\PYZus{}params\PYZus{}gb\PYZus{}smote\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search\PYZus{}gb}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best Parameters:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{best\PYZus{}params\PYZus{}gb\PYZus{}smote\PYZus{}classifier}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model Prediction using the best estimator}
\PY{n}{y\PYZus{}smote\PYZus{}pred} \PY{o}{=} \PY{n}{best\PYZus{}gb\PYZus{}smote\PYZus{}classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}smote\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model evaluation}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Classification Report:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Confusion Matrix:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ROC\PYZhy{}AUC Score:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best Parameters: \{'learning\_rate': 0.2, 'max\_depth': 7, 'n\_estimators': 200\}
Accuracy: 0.9792099792099792

Classification Report:
               precision    recall  f1-score   support

           0       0.99      0.97      0.98       481
           1       0.97      0.99      0.98       481

    accuracy                           0.98       962
   macro avg       0.98      0.98      0.98       962
weighted avg       0.98      0.98      0.98       962


Confusion Matrix:
 [[468  13]
 [  7 474]]

ROC-AUC Score:
 0.9792099792099792
    \end{Verbatim}

    \textbf{\emph{Model Performance Metrics: Our Gradient Boosting Decision
Tree - Classifier using SMOTE data achieved an accuracy of 97.92\%,
precision of 0.98, recall of 0.98, f1-score 0.98 and an ROC-AUC score of
0.9792099792099792 .}}

    \subsubsection{5. XGBoost Classifier}\label{xgboost-classifier}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Define the parameter grid for grid search}
\PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subsample}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.8}\PY{p}{,} \PY{l+m+mf}{0.9}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{colsample\PYZus{}bytree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.8}\PY{p}{,} \PY{l+m+mf}{0.9}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}
\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Create an XGBoost classifier}
\PY{n}{xgb\PYZus{}classifier} \PY{o}{=} \PY{n}{XGBClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,} \PY{n}{use\PYZus{}label\PYZus{}encoder}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{eval\PYZus{}metric}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{logloss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Perform grid search cross\PYZhy{}validation}
\PY{n}{grid\PYZus{}search\PYZus{}xgb} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{estimator}\PY{o}{=}\PY{n}{xgb\PYZus{}classifier}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{grid\PYZus{}search\PYZus{}xgb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Get the best estimator and its hyperparameters}
\PY{n}{best\PYZus{}xgb\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search\PYZus{}xgb}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
\PY{n}{best\PYZus{}params\PYZus{}xgb\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search\PYZus{}xgb}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best Parameters:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{best\PYZus{}params\PYZus{}xgb\PYZus{}classifier}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model prediction using the best estimator}
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{best\PYZus{}xgb\PYZus{}classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model evaluation}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Classification Report:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Confusion Matrix:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ROC\PYZhy{}AUC Score:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best Parameters: \{'colsample\_bytree': 0.8, 'learning\_rate': 0.1, 'max\_depth': 5,
'n\_estimators': 200, 'subsample': 0.8\}
Accuracy: 0.9526315789473684

Classification Report:
               precision    recall  f1-score   support

           0       0.97      0.98      0.97       481
           1       0.87      0.82      0.84        89

    accuracy                           0.95       570
   macro avg       0.92      0.90      0.91       570
weighted avg       0.95      0.95      0.95       570


Confusion Matrix:
 [[470  11]
 [ 16  73]]

ROC-AUC Score:
 0.8986778481160503
    \end{Verbatim}

    \textbf{\emph{Model Performance Metrics: Our XGBoost Classifier achieved
an accuracy of 95.26\%, precision of 0.95, recall of 0.95, f1-score 0.95
and an ROC-AUC score of 0.8986778481160503.}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Model training with scaled smote input data}
\PY{n}{grid\PYZus{}search\PYZus{}xgb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}smote\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}smote\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Get the best estimator and its hyperparameters}
\PY{n}{best\PYZus{}xgb\PYZus{}smote\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search\PYZus{}xgb}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
\PY{n}{best\PYZus{}params\PYZus{}xgb\PYZus{}smote\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search\PYZus{}xgb}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best Parameters:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{best\PYZus{}params\PYZus{}xgb\PYZus{}smote\PYZus{}classifier}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model Prediction using the best estimator}
\PY{n}{y\PYZus{}smote\PYZus{}pred} \PY{o}{=} \PY{n}{best\PYZus{}xgb\PYZus{}smote\PYZus{}classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}smote\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Model evaluation}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Classification Report:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Confusion Matrix:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ROC\PYZhy{}AUC Score:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best Parameters: \{'colsample\_bytree': 1.0, 'learning\_rate': 0.1, 'max\_depth': 7,
'n\_estimators': 200, 'subsample': 0.9\}
Accuracy: 0.974012474012474

Classification Report:
               precision    recall  f1-score   support

           0       0.98      0.97      0.97       481
           1       0.97      0.98      0.97       481

    accuracy                           0.97       962
   macro avg       0.97      0.97      0.97       962
weighted avg       0.97      0.97      0.97       962


Confusion Matrix:
 [[465  16]
 [  9 472]]

ROC-AUC Score:
 0.974012474012474
    \end{Verbatim}

    \textbf{\emph{Model Performance Metrics: Our XGBoost Classifier using
SMOTE data achieved an accuracy of 97.40\%, precision of 0.97, recall of
0.97, f1-score 0.97 and an ROC-AUC score of 0.974012474012474.}}

    \subsection{Model Analysis}\label{model-analysis}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} List of best models from grid search CV}
\PY{n}{best\PYZus{}models} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Logistic Regression}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{log\PYZus{}model}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{K\PYZhy{}Nearest Neighbors}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{knn}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Random Forest}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{best\PYZus{}rf\PYZus{}classifier}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gradient Boosting DT}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{best\PYZus{}gb\PYZus{}classifier}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{XGBoost}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{best\PYZus{}xgb\PYZus{}classifier}
\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Prepare a results dictionary}
\PY{n}{results} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Model}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ROC\PYZhy{}AUC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Precision}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Recall}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F1\PYZhy{}Score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}
\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Evaluate each best model}
\PY{k}{for} \PY{n}{name}\PY{p}{,} \PY{n}{model} \PY{o+ow}{in} \PY{n}{best\PYZus{}models}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
    
    \PY{n}{accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
    \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} 
    \PY{n}{report} \PY{o}{=} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{,} \PY{n}{output\PYZus{}dict}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{results}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Model}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{name}\PY{p}{)}
    \PY{n}{results}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}
    \PY{n}{results}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ROC\PYZhy{}AUC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{roc\PYZus{}auc}\PY{p}{)}
    \PY{n}{results}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Precision}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{report}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{weighted avg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{precision}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
    \PY{n}{results}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Recall}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{report}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{weighted avg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{recall}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
    \PY{n}{results}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F1\PYZhy{}Score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{report}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{weighted avg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{f1\PYZhy{}score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Convert results dictionary to DataFrame}
\PY{n}{results\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{results}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Print the results table}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{results\PYZus{}df}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                  Model  Accuracy   ROC-AUC  Precision    Recall  F1-Score
0   Logistic Regression  0.900000  0.725560   0.893407  0.900000  0.888731
1   K-Nearest Neighbors  0.947368  0.900138   0.947368  0.947368  0.947368
2         Random Forest  0.959649  0.893679   0.958980  0.959649  0.958329
3  Gradient Boosting DT  0.970175  0.931965   0.969747  0.970175  0.969822
4               XGBoost  0.952632  0.898678   0.951772  0.952632  0.952070
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} List of best models from grid search CV}
\PY{n}{best\PYZus{}models} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Logistic Regression}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{log\PYZus{}smote\PYZus{}model}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{K\PYZhy{}Nearest Neighbors}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{knn\PYZus{}smote}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Random Forest}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{best\PYZus{}rf\PYZus{}smote\PYZus{}classifier}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gradient Boosting DT}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{best\PYZus{}gb\PYZus{}smote\PYZus{}classifier}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{XGBoost}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{best\PYZus{}xgb\PYZus{}smote\PYZus{}classifier}
\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Prepare a results dictionary}
\PY{n}{results} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Smote data Model}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ROC\PYZhy{}AUC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Precision}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Recall}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F1\PYZhy{}Score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}
\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Evaluate each best model}
\PY{k}{for} \PY{n}{name}\PY{p}{,} \PY{n}{model} \PY{o+ow}{in} \PY{n}{best\PYZus{}models}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{y\PYZus{}smote\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}smote\PYZus{}test}\PY{p}{)}
    
    \PY{n}{accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)}
    \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{)} 
    \PY{n}{report} \PY{o}{=} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}smote\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}smote\PYZus{}pred}\PY{p}{,} \PY{n}{output\PYZus{}dict}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{results}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Smote data Model}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{name}\PY{p}{)}
    \PY{n}{results}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}
    \PY{n}{results}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ROC\PYZhy{}AUC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{roc\PYZus{}auc}\PY{p}{)}
    \PY{n}{results}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Precision}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{report}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{weighted avg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{precision}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
    \PY{n}{results}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Recall}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{report}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{weighted avg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{recall}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
    \PY{n}{results}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F1\PYZhy{}Score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{report}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{weighted avg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{f1\PYZhy{}score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Convert results dictionary to DataFrame}
\PY{n}{results\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{results}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Print the results table}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{results\PYZus{}df}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
       Smote data Model  Accuracy   ROC-AUC  Precision    Recall  F1-Score
0   Logistic Regression  0.837838  0.837838   0.837861  0.837838  0.837835
1   K-Nearest Neighbors  0.954262  0.954262   0.955807  0.954262  0.954223
2         Random Forest  0.974012  0.974012   0.974178  0.974012  0.974010
3  Gradient Boosting DT  0.979210  0.979210   0.979285  0.979210  0.979209
4               XGBoost  0.974012  0.974012   0.974113  0.974012  0.974011
    \end{Verbatim}

    \subsection{Conclusion and
Recommendations}\label{conclusion-and-recommendations}

    Among all the classifier models tested, the Gradient Boosting Decision
Tree model performed exceptionally well in predicting customer churn.

For the scaled input data, the model achieved an accuracy of 0.97, an
ROC-AUC of 0.93, and an F1-score of 0.96, indicating high precision and
recall. When applied to the SMOTE data, which addresses class imbalance
by oversampling the minority class, the model maintained an accuracy of
0.97, with an improved ROC-AUC of 0.97 and an F1-score of 0.97. These
metrics reflect the model's robustness and effectiveness in both
balanced and imbalanced datasets, making it a reliable choice for
predicting customer churn.

\textbf{Recommendations to Avoid Customer Churn}

Enhance Customer Engagement: Implement personalized communication and
exclusive promotions for high-value customers.

Improve Service Quality: Monitor and reduce call failures and address
complaints promptly with exceptional customer support.

Analyze and Act on Feedback: Regularly collect and analyze feedback to
identify and address common pain points.

Predictive Analytics for Proactive Measures: Use predictive analytics to
identify early signs of churn and implement targeted retention
strategies.

Flexible and Competitive Tariff Plans: Offer a range of flexible plans
and regularly adjust pricing to remain competitive.

Customer Value Enhancement: Provide additional benefits and educate
customers on maximizing the value of their plans.

Data-Driven Decision Making: Leverage data analytics to continuously
refine customer engagement and retention efforts. By implementing these
recommendations, telecom companies can reduce customer churn, enhance
customer satisfaction, and maintain a competitive edge.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
